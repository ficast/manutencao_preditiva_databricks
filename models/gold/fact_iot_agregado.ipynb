{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact IoT Agregado (Horário)\n\nAgregação horária de leituras de sensores para otimização de queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração (lê parâmetros do job)\n",
    "try:\n",
    "    CATALOG = dbutils.widgets.get('catalog')\n",
    "except:\n",
    "    CATALOG = 'manufatura_lakehouse'\n",
    "\n",
    "try:\n",
    "    SCHEMA_SILVER = dbutils.widgets.get('schema_silver')\n",
    "except:\n",
    "    SCHEMA_SILVER = 'silver'\n",
    "\n",
    "try:\n",
    "    SCHEMA_GOLD = dbutils.widgets.get('schema_gold')\n",
    "except:\n",
    "    SCHEMA_GOLD = 'gold'\n",
    "\n",
    "def fqtn(schema, table):\n",
    "    if CATALOG and CATALOG.lower() != 'none':\n",
    "        return f\"`{CATALOG}`.`{schema}`.`{table}`\"\n",
    "    else:\n",
    "        return f\"`{schema}`.`{table}`\"\n",
    "\n",
    "# Criar schema se necessário\n",
    "if CATALOG and CATALOG.lower() != 'none':\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{SCHEMA_GOLD}`\")\n",
    "    spark.sql(f\"USE CATALOG `{CATALOG}`\")\n",
    "else:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{SCHEMA_GOLD}`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {fqtn(SCHEMA_GOLD, 'fact_iot_agregado')} (\n",
    "    equipment_id STRING,\n",
    "    sensor_type STRING,\n",
    "    hour_bucket TIMESTAMP,\n",
    "    date_key INT,\n",
    "    avg_value DOUBLE,\n",
    "    min_value DOUBLE,\n",
    "    max_value DOUBLE,\n",
    "    reading_count BIGINT\n",
    ") USING DELTA\n",
    "PARTITIONED BY (date_key, sensor_type)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "INSERT OVERWRITE {fqtn(SCHEMA_GOLD, 'fact_iot_agregado')}\n",
    "SELECT \n",
    "    equipment_id,\n",
    "    sensor_type,\n",
    "    date_trunc('hour', reading_timestamp) as hour_bucket,\n",
    "    cast(date_format(date_trunc('hour', reading_timestamp), 'yyyyMMdd') as int) as date_key,\n",
    "    avg(reading_value) as avg_value,\n",
    "    min(reading_value) as min_value,\n",
    "    max(reading_value) as max_value,\n",
    "    count(*) as reading_count\n",
    "FROM {fqtn(SCHEMA_SILVER, 'iot_readings_clean')}\n",
    "GROUP BY 1, 2, 3\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}