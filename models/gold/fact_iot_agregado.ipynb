{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact IoT Agregado (Horário)\n\nAgregação horária de leituras de sensores para otimização de queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {fqtn(SCHEMA_GOLD, 'fact_iot_agregado')} (\n",
    "    equipment_id STRING,\n",
    "    sensor_type STRING,\n",
    "    hour_bucket TIMESTAMP,\n",
    "    date_key INT,\n",
    "    avg_value DOUBLE,\n",
    "    min_value DOUBLE,\n",
    "    max_value DOUBLE,\n",
    "    reading_count BIGINT\n",
    ") USING DELTA\n",
    "PARTITIONED BY (date_key, sensor_type)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "INSERT OVERWRITE {fqtn(SCHEMA_GOLD, 'fact_iot_agregado')}\n",
    "SELECT \n",
    "    equipment_id,\n",
    "    sensor_type,\n",
    "    date_trunc('hour', reading_timestamp) as hour_bucket,\n",
    "    cast(date_format(date_trunc('hour', reading_timestamp), 'yyyyMMdd') as int) as date_key,\n",
    "    avg(reading_value) as avg_value,\n",
    "    min(reading_value) as min_value,\n",
    "    max(reading_value) as max_value,\n",
    "    count(*) as reading_count\n",
    "FROM {fqtn(SCHEMA_SILVER, 'iot_readings_clean')}\n",
    "GROUP BY 1, 2, 3\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}